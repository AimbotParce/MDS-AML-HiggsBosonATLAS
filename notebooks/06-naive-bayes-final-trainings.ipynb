{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3801ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and prepare the clean variables.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"../data/processed/train_no_preprocess.csv\")\n",
    "\n",
    "X_train = df_train.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_train = df_train[\"Label\"]\n",
    "weights_train = df_train[\"Weight\"]\n",
    "categorical_features = df_train.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_train_drop_rows = df_train[~X_train.isna().any(axis=1)].reset_index(drop=True)\n",
    "X_train_drop_rows = df_train_drop_rows.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_train_drop_rows = df_train_drop_rows[\"Label\"]\n",
    "weights_train_drop_rows = df_train_drop_rows[\"Weight\"]\n",
    "categorical_features_drop_rows = df_train_drop_rows.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "cols_dropped = X_train.columns[X_train.isna().any(axis=0)]\n",
    "df_train_drop_cols = df_train.drop(columns=cols_dropped)\n",
    "X_train_drop_cols = df_train_drop_cols.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_train_drop_cols = df_train_drop_cols[\"Label\"]\n",
    "weights_train_drop_cols = df_train_drop_cols[\"Weight\"]\n",
    "categorical_features_drop_cols = df_train_drop_cols.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"../data/processed/test_no_preprocess.csv\")\n",
    "\n",
    "X_test = df_test.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_test = df_test[\"Label\"]\n",
    "weights_test = df_test[\"Weight\"]\n",
    "categorical_features = df_test.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_test_drop_rows = df_test[~X_test.isna().any(axis=1)].reset_index(drop=True)\n",
    "X_test_drop_rows = df_test_drop_rows.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_test_drop_rows = df_test_drop_rows[\"Label\"]\n",
    "weights_test_drop_rows = df_test_drop_rows[\"Weight\"]\n",
    "categorical_features_drop_rows = df_test_drop_rows.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_test_drop_cols = df_test.drop(columns=cols_dropped)\n",
    "X_test_drop_cols = df_test_drop_cols.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_test_drop_cols = df_test_drop_cols[\"Label\"]\n",
    "weights_test_drop_cols = df_test_drop_cols[\"Weight\"]\n",
    "categorical_features_drop_cols = df_test_drop_cols.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "# Convert them to numpy and store them in a datasets dictionary for easy reference.\n",
    "datasets = {\n",
    "    \"original\": (\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "        weights_train.to_numpy(),\n",
    "        X_test.to_numpy(),\n",
    "        y_test.to_numpy(),\n",
    "        weights_test.to_numpy(),\n",
    "        categorical_features,\n",
    "    ),\n",
    "    \"drop-rows\": (\n",
    "        X_train_drop_rows.to_numpy(),\n",
    "        y_train_drop_rows.to_numpy(),\n",
    "        weights_train_drop_rows.to_numpy(),\n",
    "        X_test_drop_rows.to_numpy(),\n",
    "        y_test_drop_rows.to_numpy(),\n",
    "        weights_test_drop_rows.to_numpy(),\n",
    "        categorical_features_drop_rows,\n",
    "    ),\n",
    "    \"drop-columns\": (\n",
    "        X_train_drop_cols.to_numpy(),\n",
    "        y_train_drop_cols.to_numpy(),\n",
    "        weights_train_drop_cols.to_numpy(),\n",
    "        X_test_drop_cols.to_numpy(),\n",
    "        y_test_drop_cols.to_numpy(),\n",
    "        weights_test_drop_cols.to_numpy(),\n",
    "        categorical_features_drop_cols,\n",
    "    ),\n",
    "}\n",
    "del (\n",
    "    df_train,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    df_train_drop_rows,\n",
    "    X_train_drop_rows,\n",
    "    y_train_drop_rows,\n",
    "    df_train_drop_cols,\n",
    "    X_train_drop_cols,\n",
    "    y_train_drop_cols,\n",
    "    categorical_features,\n",
    "    categorical_features_drop_rows,\n",
    "    categorical_features_drop_cols,\n",
    "    df_test,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    df_test_drop_rows,\n",
    "    X_test_drop_rows,\n",
    "    y_test_drop_rows,\n",
    "    df_test_drop_cols,\n",
    "    X_test_drop_cols,\n",
    "    y_test_drop_cols,\n",
    "    weights_test_drop_cols,\n",
    ")  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5936819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.naive_bayes.experiments import ExperimentBase, run_all_experiments, load_experiment_set\n",
    "\n",
    "\n",
    "class FinalExperiment(ExperimentBase):\n",
    "    def _get_train_test_data(self, datasets):\n",
    "        X_train, y_train, weights_train, X_test, y_test, weights_test, categorical_features = datasets[self.dataset]\n",
    "        return X_train, y_train, weights_train, X_test, y_test, weights_test, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de408a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "final_experiments = load_experiment_set(FinalExperiment, \"../results/final-experiments.jsonl\")\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    run_all_experiments(\n",
    "        experiments=set(final_experiments),\n",
    "        datasets=datasets,\n",
    "        results_file=\"../results/final-experiments-results.jsonl\",\n",
    "        failed_file=\"../results/final-experiments-failed.jsonl\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba63ba",
   "metadata": {},
   "source": [
    "For the report, let us print all the results in a $\\LaTeX$ table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decf4b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table lines for final experiment results:\n",
      "Gaussian NB  (Drop Missing Rows)& 0.43 & - & 0.74 & 0.74 \\\\\n",
      "Gaussian NB  (Drop Missing Columns)& 0.61 & - & 0.72 & 0.68 \\\\\n",
      "Histogram NB  (Drop Missing Rows)& 0.54 & - & 0.74 & 0.72 \\\\\n",
      "Histogram NB  (Drop Missing Columns)& 0.72 & - & 0.67 & 0.67 \\\\\n",
      "Eager Gaussian KDE NB  (Drop Missing Rows)& 0.50 & - & 0.79 & 0.79 \\\\\n",
      "Eager Gaussian KDE NB  (Drop Missing Columns)& 0.75 & - & 0.74 & 0.71 \\\\\n",
      "Yeo-Johnson Gaussian NB  (Drop Missing Rows)& 0.25 & - & 0.58 & 0.46 \\\\\n",
      "Yeo-Johnson Gaussian NB  (Drop Missing Columns)& 0.03 & - & 0.66 & 0.40 \\\\\n",
      "Robust Gaussian NB & 0.60 & - & 0.74 & 0.68 \\\\\n",
      "Robust Histogram NB & 0.76 & - & 0.72 & 0.71 \\\\\n",
      "Robust Eager Gaussian KDE NB & 0.60 & - & 0.72 & 0.69 \\\\\n",
      "Robust Yeo-Johnson Gaussian NB & 0.20 & - & 0.67 & 0.43 \\\\\n",
      "Robust Gaussian NB with Categorical Dependency& 0.59 & - & 0.74 & 0.69 \\\\\n",
      "Robust Histogram NB with Categorical Dependency& 0.68 & - & 0.74 & 0.72 \\\\\n",
      "Robust Eager Gaussian KDE NB with Categorical Dependency& 0.86 & - & 0.79 & 0.77 \\\\\n",
      "Robust Yeo-Johnson Gaussian NB with Categorical Dependency& 0.48 & - & 0.65 & 0.55 \\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Gaussian NB with \n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Histogram NB with \n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Eager Gaussian KDE NB with \n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Yeo-Johnson Gaussian NB with \n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Gaussian NB with with Categorical Dependency\n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Histogram NB with with Categorical Dependency\n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Eager Gaussian KDE NB with with Categorical Dependency\n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_24948\\117625095.py:37: UserWarning: No results for Yeo-Johnson Gaussian NB with with Categorical Dependency\n",
      "  warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "final_experiment_results = load_experiment_set(FinalExperiment, \"../results/final-experiments-results.jsonl\")\n",
    "\n",
    "final_latex_table_lines = []\n",
    "for model_class, model_name in [\n",
    "    (\"BespokeNB\", \"\"),\n",
    "    (\"CategoricalAwareBespokeNB\", \"with Categorical Dependency\"),\n",
    "]:\n",
    "    for continuous_estimator_class, estimator_name in [\n",
    "        (\"GaussianEstimator\", \"Gaussian\"),\n",
    "        (\"HistogramEstimator\", \"Histogram\"),\n",
    "        (\"EagerGaussianKDEstimator\", \"Eager Gaussian KDE\"),\n",
    "        (\"YeoJohnsonGaussianEstimator\", \"Yeo-Johnson Gaussian\"),\n",
    "        (\"RobustGaussianEstimator\", \"Robust Gaussian\"),\n",
    "        (\"RobustHistogramEstimator\", \"Robust Histogram\"),\n",
    "        (\"RobustEagerGaussianKDEstimator\", \"Robust Eager Gaussian KDE\"),\n",
    "        (\"RobustYeoJohnsonGaussianEstimator\", \"Robust Yeo-Johnson Gaussian\"),\n",
    "    ]:\n",
    "        for dataset, dataset_name in [\n",
    "            (\"original\", \"\"),\n",
    "            (\"drop-rows\", \" (Drop Missing Rows)\"),\n",
    "            (\"drop-columns\", \" (Drop Missing Columns)\"),\n",
    "        ]:\n",
    "            if \"Robust\" in continuous_estimator_class and dataset != \"original\":\n",
    "                # Ignore these results, as they make little sense\n",
    "                continue\n",
    "            subset = list(\n",
    "                filter(\n",
    "                    lambda ex: (ex.model_class == model_class)\n",
    "                    & (ex.continuous_estimator_class == continuous_estimator_class)\n",
    "                    & (ex.dataset == dataset),\n",
    "                    final_experiment_results,\n",
    "                )\n",
    "            )\n",
    "            if len(subset) == 0:\n",
    "                warnings.warn(f\"No results for {estimator_name} NB with {model_name}\")\n",
    "                continue\n",
    "            best = max(subset, key=lambda ex: ex.result.ams_score)\n",
    "            macro_f1 = (best.result.b_f1_score + best.result.s_f1_score) / 2\n",
    "            final_latex_table_lines.append(\n",
    "                f\"{estimator_name} NB {model_name}{dataset_name}& \"\n",
    "                f\"{best.result.ams_score:.2f} & \"\n",
    "                \"- & \"\n",
    "                f\"{best.result.accuracy:.2f} & \"\n",
    "                f\"{macro_f1:.2f} \\\\\\\\\"\n",
    "            )\n",
    "\n",
    "print(\"LaTeX table lines for final experiment results:\")\n",
    "for line in final_latex_table_lines:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgsbosonatlas (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
