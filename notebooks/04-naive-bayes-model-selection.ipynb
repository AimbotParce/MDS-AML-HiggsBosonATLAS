{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eded411",
   "metadata": {},
   "source": [
    "## Naive Bayes Model Selection\n",
    "\n",
    "We have implemented several different variants of the Naive Bayes classifier, each with its own assumptions and characteristics. To determine which model performs best for our specific dataset, we will conduct a model selection process using some resampling technique.\n",
    "\n",
    "### Choice of Resampling Technique\n",
    "\n",
    "By default, we will use k-fold cross-validation for model selection and hyperparameter tuning. However, in some algorithms, training is extremely costly, and in such cases, because we have a large dataset, we may opt for a simple train-validation split instead. The choice of resampling technique will be made based on the computational cost of training each model.\n",
    "\n",
    "Note that in all cases, the test set will remain untouched until the final evaluation phase.\n",
    "\n",
    "### Model Space\n",
    "\n",
    "We will consider the following Naive Bayes variants for model selection:\n",
    "1. Gaussian Naive Bayes Variants:\n",
    "   1. Gaussian Naive Bayes (As a baseline model): The standard Gaussian Naive Bayes model that assumes features are normally distributed.\n",
    "      1. Dropping rows with missing values\n",
    "      2. Dropping features with missing values\n",
    "   2. Robust Gaussian Naive Bayes: A variant of the Gaussian Naive Bayes that uses the missingness of a feature as an additional categorical feature. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Gaussian Naive Bayes with: A variant of the Robust Gaussian Naive Bayes that treats continuous features as independent of one another (as does the standard Naive Bayes model) but dependent on the categorical features (not the missingness indicators). (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "2. Histogram-Based Naive Bayes Variants:\n",
    "   1. Histogram Naive Bayes: A Naive Bayes model that uses histograms to estimate the probability density functions of continuous features.\n",
    "      1. Dropping rows with missing values (`bins in range(10, 1000, 10) + [None]`)\n",
    "      2. Dropping features with missing values (`bins in range(10, 1000, 10) + [None]`)\n",
    "   2. Robust Histogram Naive Bayes: A variant of the Histogram Naive Bayes that incorporates missingness indicators as additional categorical features. (`bins in range(10, 1000, 10) + [None]`, `laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Histogram Naive Bayes: A variant of the Robust Histogram Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`bins in range(10, 1000, 10) + [None]`, `laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "3. Kernel Density Estimation (KDE) Based Naive Bayes Variants:\n",
    "   1. Gaussian KDE Naive Bayes: A Naive Bayes model that uses Kernel Density Estimation with a Gaussian Kernel to estimate the probability density functions of continuous features.\n",
    "      1. Dropping rows with missing values (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`)\n",
    "      2. Dropping features with missing values (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`)\n",
    "   2. Robust Gaussian KDE Naive Bayes: A variant of the KDE Naive Bayes that incorporates missingness indicators as additional categorical features. (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`, `laplace_smoothing in <about_the_best_as_in_previous_models> + [0]`)\n",
    "   3. Categorical-Aware Robust Gaussian KDE Naive Bayes: A variant of the Robust KDE Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`, `laplace_smoothing in <about_the_best_as_in_previous_models> + [0]`)\n",
    "   > **Note:** KDE models are by design lazily evaluated, meaning that training consists on just storing all the training data, and actual density estimation is performed at prediction time. Therefore, they are extremely costly to evaluate. For that reason, an **approximation** is used instead, where a large-enough sample of the feature axis is drawn and the density is estimated at those points only, using them to approximate the density at prediction time via 1-nearest-neighbor.\n",
    "4. Yeo-Johnson Transformed Gaussian Naive Bayes Variants:\n",
    "   1. Yeo-Johnson Transformed Gaussian Naive Bayes: A Naive Bayes model that applies a Yeo-Johnson transformation to continuous features before modeling them with Gaussian distributions.\n",
    "      1. Dropping rows with missing values\n",
    "      2. Dropping features with missing values\n",
    "   2. Robust Yeo-Johnson Transformed Gaussian Naive Bayes: A variant of the Yeo-Johnson Transformed Gaussian Naive Bayes that incorporates missingness indicators as additional categorical features. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Yeo-Johnson Transformed Gaussian Naive Bayes: A variant of the Robust Yeo-Johnson Transformed Gaussian Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "\n",
    "\n",
    "> **Note:** In all cases, categorical features are modeled using a multi-class generalization of the Bernoulli distribution ($f(x=i|\\vec{p})=p_i$), while continuous features are modeled using different Density Estimation techniques (Gaussian, KDE, or Histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0c0ca5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DER_mass_MMC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_transverse_met_lep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_vis",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_deltaeta_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_prodeta_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_deltar_tau_lep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_tot",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_sum_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_ratio_lep_tau",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_met_phi_centrality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_lep_eta_centrality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met_sumet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PRI_jet_leading_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_leading_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_leading_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_all_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cedec1a8-f3c1-4a17-b415-74c37f8925e4",
       "rows": [
        [
         "0",
         "90.901",
         "85.57",
         "75.316",
         "40.945",
         null,
         null,
         null,
         "1.869",
         "3.111",
         "135.816",
         "1.073",
         "-1.06",
         null,
         "44.964",
         "-0.972",
         "0.788",
         "48.247",
         "-0.738",
         "-1.066",
         "37.976",
         "2.136",
         "185.052",
         "1",
         "42.605",
         "-1.962",
         "-2.519",
         null,
         null,
         null,
         "42.605",
         "0.58939479024",
         "b"
        ],
        [
         "1",
         "133.477",
         "3.669",
         "99.223",
         "227.121",
         "2.243",
         "365.016",
         "2.278",
         "1.223",
         "3.539",
         "440.917",
         "2.495",
         "0.949",
         "0.094",
         "51.602",
         "-0.978",
         "0.509",
         "128.748",
         "-0.155",
         "-0.395",
         "63.331",
         "-0.436",
         "453.808",
         "2",
         "173.249",
         "-0.759",
         "2.545",
         "87.317",
         "-3.002",
         "-2.594",
         "260.566",
         "0.000461281573949",
         "s"
        ],
        [
         "2",
         "115.111",
         "26.919",
         "77.658",
         "50.266",
         null,
         null,
         null,
         "2.691",
         "3.655",
         "133.495",
         "1.398",
         "1.398",
         null,
         "33.191",
         "-0.614",
         "-1.834",
         "46.409",
         "-0.248",
         "1.783",
         "27.559",
         "2.555",
         "176.401",
         "1",
         "53.895",
         "0.685",
         "-0.613",
         null,
         null,
         null,
         "53.895",
         "0.623626505663",
         "b"
        ],
        [
         "3",
         null,
         "83.642",
         "74.642",
         "25.176",
         null,
         null,
         null,
         "2.646",
         "25.176",
         "53.813",
         "1.393",
         "-1.361",
         null,
         "22.488",
         "0.205",
         "-1.051",
         "31.326",
         "2.215",
         "0.671",
         "60.143",
         "-3.012",
         "77.408",
         "0",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "1.77211428467",
         "b"
        ],
        [
         "4",
         "81.958",
         "7.074",
         "46.894",
         "90.979",
         "0.952",
         "83.883",
         "-0.226",
         "1.626",
         "17.517",
         "174.686",
         "1.05",
         "1.171",
         "0.001",
         "29.799",
         "0.486",
         "2.777",
         "31.299",
         "1.271",
         "1.353",
         "47.727",
         "1.536",
         "265.013",
         "2",
         "80.028",
         "-0.456",
         "-1.902",
         "33.561",
         "0.496",
         "-0.555",
         "113.589",
         "0.418760425129",
         "b"
        ]
       ],
       "shape": {
        "columns": 32,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.901</td>\n",
       "      <td>85.570</td>\n",
       "      <td>75.316</td>\n",
       "      <td>40.945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.869</td>\n",
       "      <td>3.111</td>\n",
       "      <td>135.816</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>42.605</td>\n",
       "      <td>-1.962</td>\n",
       "      <td>-2.519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.605</td>\n",
       "      <td>0.589395</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133.477</td>\n",
       "      <td>3.669</td>\n",
       "      <td>99.223</td>\n",
       "      <td>227.121</td>\n",
       "      <td>2.243</td>\n",
       "      <td>365.016</td>\n",
       "      <td>2.278</td>\n",
       "      <td>1.223</td>\n",
       "      <td>3.539</td>\n",
       "      <td>440.917</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>173.249</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>2.545</td>\n",
       "      <td>87.317</td>\n",
       "      <td>-3.002</td>\n",
       "      <td>-2.594</td>\n",
       "      <td>260.566</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.111</td>\n",
       "      <td>26.919</td>\n",
       "      <td>77.658</td>\n",
       "      <td>50.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.691</td>\n",
       "      <td>3.655</td>\n",
       "      <td>133.495</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.895</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.895</td>\n",
       "      <td>0.623627</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>83.642</td>\n",
       "      <td>74.642</td>\n",
       "      <td>25.176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.646</td>\n",
       "      <td>25.176</td>\n",
       "      <td>53.813</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.772114</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.958</td>\n",
       "      <td>7.074</td>\n",
       "      <td>46.894</td>\n",
       "      <td>90.979</td>\n",
       "      <td>0.952</td>\n",
       "      <td>83.883</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>1.626</td>\n",
       "      <td>17.517</td>\n",
       "      <td>174.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80.028</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>33.561</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>113.589</td>\n",
       "      <td>0.418760</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0        90.901                       85.570        75.316    40.945   \n",
       "1       133.477                        3.669        99.223   227.121   \n",
       "2       115.111                       26.919        77.658    50.266   \n",
       "3           NaN                       83.642        74.642    25.176   \n",
       "4        81.958                        7.074        46.894    90.979   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                   NaN               NaN                  NaN   \n",
       "1                 2.243           365.016                2.278   \n",
       "2                   NaN               NaN                  NaN   \n",
       "3                   NaN               NaN                  NaN   \n",
       "4                 0.952            83.883               -0.226   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  PRI_jet_num  \\\n",
       "0               1.869       3.111     135.816  ...            1   \n",
       "1               1.223       3.539     440.917  ...            2   \n",
       "2               2.691       3.655     133.495  ...            1   \n",
       "3               2.646      25.176      53.813  ...            0   \n",
       "4               1.626      17.517     174.686  ...            2   \n",
       "\n",
       "   PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0              42.605               -1.962               -2.519   \n",
       "1             173.249               -0.759                2.545   \n",
       "2              53.895                0.685               -0.613   \n",
       "3                 NaN                  NaN                  NaN   \n",
       "4              80.028               -0.456               -1.902   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                    NaN                     NaN                     NaN   \n",
       "1                 87.317                  -3.002                  -2.594   \n",
       "2                    NaN                     NaN                     NaN   \n",
       "3                    NaN                     NaN                     NaN   \n",
       "4                 33.561                   0.496                  -0.555   \n",
       "\n",
       "   PRI_jet_all_pt    Weight  Label  \n",
       "0          42.605  0.589395      b  \n",
       "1         260.566  0.000461      s  \n",
       "2          53.895  0.623627      b  \n",
       "3           0.000  1.772114      b  \n",
       "4         113.589  0.418760      b  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and prepare the clean variables.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/train_no_preprocess.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9ca716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Label\", \"Weight\"])\n",
    "y = df[\"Label\"]\n",
    "weights = df[\"Weight\"]\n",
    "categorical_features = df.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_drop_rows = df[~X.isna().any(axis=1)].reset_index(drop=True)\n",
    "X_drop_rows = df_drop_rows.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_drop_rows = df_drop_rows[\"Label\"]\n",
    "weights_drop_rows = df_drop_rows[\"Weight\"]\n",
    "categorical_features_drop_rows = df_drop_rows.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_drop_cols = df.drop(columns=X.columns[X.isna().any(axis=0)])\n",
    "X_drop_cols = df_drop_cols.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_drop_cols = df_drop_cols[\"Label\"]\n",
    "weights_drop_cols = df_drop_cols[\"Weight\"]\n",
    "categorical_features_drop_cols = df_drop_cols.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "# Convert them to numpy and store them in a datasets dictionary for easy reference.\n",
    "datasets = {\n",
    "    \"original\": (X.to_numpy(), y.to_numpy(), weights.to_numpy(), categorical_features),\n",
    "    \"drop-rows\": (\n",
    "        X_drop_rows.to_numpy(),\n",
    "        y_drop_rows.to_numpy(),\n",
    "        weights_drop_rows.to_numpy(),\n",
    "        categorical_features_drop_rows,\n",
    "    ),\n",
    "    \"drop-columns\": (\n",
    "        X_drop_cols.to_numpy(),\n",
    "        y_drop_cols.to_numpy(),\n",
    "        weights_drop_cols.to_numpy(),\n",
    "        categorical_features_drop_cols,\n",
    "    ),\n",
    "}\n",
    "del (\n",
    "    df,\n",
    "    X,\n",
    "    y,\n",
    "    df_drop_rows,\n",
    "    X_drop_rows,\n",
    "    y_drop_rows,\n",
    "    df_drop_cols,\n",
    "    X_drop_cols,\n",
    "    y_drop_cols,\n",
    "    categorical_features,\n",
    "    categorical_features_drop_rows,\n",
    "    categorical_features_drop_cols,\n",
    ")  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d730a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, TypedDict, Literal, NewType, Union\n",
    "from pydantic import BaseModel, model_validator\n",
    "\n",
    "\n",
    "class Experiment(BaseModel):\n",
    "    model_class: Literal[\"BespokeNB\", \"CategoricalAwareBespokeNB\"]\n",
    "    categorical_estimator_class: Literal[\"CategoricalEstimator\", \"RobustCategoricalEstimator\"]\n",
    "    continuous_estimator_class: Literal[\n",
    "        \"GaussianEstimator\",\n",
    "        \"RobustGaussianEstimator\",\n",
    "        \"HistogramEstimator\",\n",
    "        \"RobustHistogramEstimator\",\n",
    "        \"EagerGaussianKDEstimator\",\n",
    "        \"RobustEagerGaussianKDEstimator\",\n",
    "        \"YeoJohnsonGaussianEstimator\",\n",
    "        \"RobustYeoJohnsonGaussianEstimator\",\n",
    "    ]\n",
    "    dataset: Literal[\"original\", \"drop-rows\", \"drop-columns\"]\n",
    "    num_folds: int\n",
    "    fold_index: int\n",
    "    categorical_estimator_params: Dict[str, Union[Optional[float], Optional[int]]] = {}\n",
    "    continuous_estimator_params: Dict[str, Union[Optional[float], Optional[int]]] = {}\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_fold_index(self):\n",
    "        if self.fold_index >= self.num_folds:\n",
    "            raise ValueError(\"fold_index must be less than num_folds\")\n",
    "        return self\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.model_class,\n",
    "                self.categorical_estimator_class,\n",
    "                self.continuous_estimator_class,\n",
    "                frozenset(self.categorical_estimator_params.items()),\n",
    "                frozenset(self.continuous_estimator_params.items()),\n",
    "                self.dataset,\n",
    "                self.num_folds,\n",
    "                self.fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Experiment):\n",
    "            return NotImplemented\n",
    "        return (\n",
    "            self.model_class == other.model_class\n",
    "            and self.categorical_estimator_class == other.categorical_estimator_class\n",
    "            and self.continuous_estimator_class == other.continuous_estimator_class\n",
    "            and self.categorical_estimator_params == other.categorical_estimator_params\n",
    "            and self.continuous_estimator_params == other.continuous_estimator_params\n",
    "            and self.dataset == other.dataset\n",
    "            and self.num_folds == other.num_folds\n",
    "            and self.fold_index == other.fold_index\n",
    "        )\n",
    "\n",
    "\n",
    "class ExperimentResult(Experiment):\n",
    "    accuracy: float\n",
    "    b_recall: float\n",
    "    b_precision: float\n",
    "    b_f1_score: float\n",
    "    s_recall: float\n",
    "    s_precision: float\n",
    "    s_f1_score: float\n",
    "    ams_score: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730dfae",
   "metadata": {},
   "source": [
    "Let's setup the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2edbcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments: List[Experiment] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48719d",
   "metadata": {},
   "source": [
    "#### Gaussian experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38d78908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                continuous_estimator_class=\"GaussianEstimator\",\n",
    "                dataset=dataset,\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49789e2c",
   "metadata": {},
   "source": [
    "#### Histogram experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bda74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for bins in list(range(10, 1000, 10)) + [None]:\n",
    "        for fold_index in range(num_folds):\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"HistogramEstimator\",\n",
    "                    dataset=dataset,\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(bins=bins),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for bins in list(range(10, 1000, 10)) + [None]:\n",
    "        for fold_index in range(num_folds):\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"RobustHistogramEstimator\",\n",
    "                    dataset=\"original\",\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing, bins=bins),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"CategoricalAwareBespokeNB\",\n",
    "                    categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"RobustHistogramEstimator\",\n",
    "                    dataset=\"original\",\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing, bins=bins),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632ae49",
   "metadata": {},
   "source": [
    "Let us skip the KDE estimators, as they are much much more computationally expensive to run, and perform first the Box-Cox experiments.\n",
    "\n",
    "#### Yeo-Johnson experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3bd5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeo-Johnson Gaussian Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                continuous_estimator_class=\"YeoJohnsonGaussianEstimator\",\n",
    "                dataset=dataset,\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustYeoJohnsonGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustYeoJohnsonGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb445fa",
   "metadata": {},
   "source": [
    "Now let's create an experiment runner function to run all the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "143bf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Type\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import src.naive_bayes\n",
    "import src.evaluate\n",
    "\n",
    "\n",
    "def _instantiate_estimator(\n",
    "    estimator_cls: Type[src.naive_bayes.ProbabilityEstimator],\n",
    "    estimator_params: Dict[str, Union[Optional[float], Optional[int]]],\n",
    ") -> src.naive_bayes.ProbabilityEstimator:\n",
    "    init_kwargs = {}\n",
    "    for key in estimator_cls.__init__.__code__.co_varnames[1:]:\n",
    "        if not key in estimator_params:\n",
    "            raise ValueError(f\"Missing value for estimator parameter: {key}\")\n",
    "        init_kwargs[key] = estimator_params[key]\n",
    "    return estimator_cls(**init_kwargs)\n",
    "\n",
    "\n",
    "def _get_estimator_instances(\n",
    "    experiment: Experiment, num_features: int, categorical_features: list[int]\n",
    ") -> Dict[int, src.naive_bayes.ProbabilityEstimator]:\n",
    "    categorical_estimator_cls = getattr(src.naive_bayes, experiment.categorical_estimator_class)\n",
    "    continuous_estimator_cls = getattr(src.naive_bayes, experiment.continuous_estimator_class)\n",
    "    instances = {}\n",
    "    for feature in range(num_features):\n",
    "        if feature in categorical_features:\n",
    "            instances[feature] = _instantiate_estimator(\n",
    "                categorical_estimator_cls, experiment.categorical_estimator_params\n",
    "            )\n",
    "        else:\n",
    "            instances[feature] = _instantiate_estimator(\n",
    "                continuous_estimator_cls, experiment.continuous_estimator_params\n",
    "            )\n",
    "    return instances\n",
    "\n",
    "\n",
    "def _get_model_instance(\n",
    "    experiment: Experiment, num_features: int, categorical_features: list[int]\n",
    ") -> src.naive_bayes.BespokeNB | src.naive_bayes.CategoricalAwareBespokeNB:\n",
    "    model_cls = getattr(src.naive_bayes, experiment.model_class)\n",
    "    estimators = _get_estimator_instances(\n",
    "        experiment, num_features=num_features, categorical_features=categorical_features\n",
    "    )\n",
    "    if model_cls == src.naive_bayes.BespokeNB:\n",
    "        return model_cls(estimators=estimators)\n",
    "    elif model_cls == src.naive_bayes.CategoricalAwareBespokeNB:\n",
    "        return model_cls(\n",
    "            estimators=estimators,\n",
    "            categorical_features=categorical_features,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model class: {experiment.model_class}\")\n",
    "\n",
    "\n",
    "def run_experiment(experiment: Experiment) -> ExperimentResult:\n",
    "    X, y, weights, categorical_features = datasets[experiment.dataset]\n",
    "\n",
    "    # Create cross-validation folds\n",
    "    kf = KFold(n_splits=experiment.num_folds, shuffle=True, random_state=42)\n",
    "    for fold_index, (train, test) in enumerate(kf.split(X)):\n",
    "        if fold_index == experiment.fold_index:\n",
    "            # Skip folds that are not the current fold index, and keep only the desired fold\n",
    "            break\n",
    "\n",
    "    model = _get_model_instance(experiment, num_features=X.shape[1], categorical_features=categorical_features)\n",
    "    model.fit(X[train], y[train])\n",
    "    y_pred = model.predict(X[test])\n",
    "    accuracy = accuracy_score(y[test], y_pred)\n",
    "    (b_precision, s_precision), (b_recall, s_recall), (b_f1_score, s_f1_score), _ = precision_recall_fscore_support(\n",
    "        y[test], y_pred, labels=[\"b\", \"s\"], average=None, zero_division=0\n",
    "    )\n",
    "    ams_score = src.evaluate.ams_score(y_true=y[test], y_pred=y_pred, weights=weights[test])\n",
    "    return ExperimentResult(\n",
    "        **experiment.model_dump(),\n",
    "        accuracy=accuracy,\n",
    "        b_recall=b_recall,\n",
    "        b_precision=b_precision,\n",
    "        b_f1_score=b_f1_score,\n",
    "        s_recall=s_recall,\n",
    "        s_precision=s_precision,\n",
    "        s_f1_score=s_f1_score,\n",
    "        ams_score=ams_score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefafeb8",
   "metadata": {},
   "source": [
    "And finally, let's run the experiments and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e916a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, TypeVar, Type\n",
    "import os\n",
    "\n",
    "T = TypeVar(\"T\", bound=Union[ExperimentResult, Experiment])\n",
    "\n",
    "\n",
    "def store_experiment_set(experiments: Set[ExperimentResult] | Set[Experiment], filename: str) -> None:\n",
    "    \"\"\"Store a set of ExperimentResult to a JSONL file.\"\"\"\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    with open(filename, \"w\") as f:\n",
    "        for er in experiments:\n",
    "            if not isinstance(er, (ExperimentResult, Experiment)):\n",
    "                raise ValueError(f\"Invalid type in experiments set: {type(er)}\")\n",
    "            f.write(er.model_dump_json() + \"\\n\")\n",
    "\n",
    "\n",
    "def load_experiment_set(cls: Type[T], filename: str) -> Set[T]:\n",
    "    \"\"\"Load a set of ExperimentResult from a JSONL file.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    experiments: Set[T] = set()\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            experiments.add(cls.model_validate_json(line.strip()))\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95b9983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "    experiments: Set[Experiment], *, verbose: bool = True, results_directory: Optional[os.PathLike] = None\n",
    ") -> Tuple[Set[ExperimentResult], Set[Experiment]]:\n",
    "    if results_directory is None:\n",
    "        results: Set[ExperimentResult] = set()\n",
    "        failed: Set[Experiment] = set()\n",
    "    else:\n",
    "        results: Set[ExperimentResult] = load_experiment_set(\n",
    "            ExperimentResult, os.path.join(results_directory, \"experiments-results.jsonl\")\n",
    "        )\n",
    "        failed: Set[Experiment] = load_experiment_set(\n",
    "            Experiment, os.path.join(results_directory, \"experiments-failed.jsonl\")\n",
    "        )\n",
    "    missing_experiments = experiments - results\n",
    "    # store_experiment_set( # For debugging only\n",
    "    #     missing_experiments,\n",
    "    #     os.path.join(results_directory, \"experiments-missing.jsonl\"),\n",
    "    # )\n",
    "    bar = tqdm(total=len(missing_experiments), desc=\"Running experiments\", disable=not verbose)\n",
    "    try:\n",
    "        for experiment in missing_experiments:\n",
    "            # Skip experiments that have already been run\n",
    "            if experiment in results:\n",
    "                bar.update(1)\n",
    "                continue\n",
    "            bar.set_postfix_str(\n",
    "                f\"{experiment.model_class} with {experiment.continuous_estimator_class} on {experiment.dataset}\"\n",
    "            )\n",
    "            bar.refresh()\n",
    "            try:\n",
    "                results.add(run_experiment(experiment))\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Experiment failed: {e}\")\n",
    "                failed.add(experiment)\n",
    "            bar.update(1)\n",
    "            if results_directory is not None:\n",
    "                # Save intermediate results\n",
    "                store_experiment_set(\n",
    "                    results,\n",
    "                    os.path.join(results_directory, \"experiments-results.jsonl\"),\n",
    "                )\n",
    "                store_experiment_set(\n",
    "                    failed,\n",
    "                    os.path.join(results_directory, \"experiments-failed.jsonl\"),\n",
    "                )\n",
    "    finally:\n",
    "        bar.close()\n",
    "    return results, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f288993",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    run_all_experiments(experiments=set(experiments), results_directory=\"../results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5a569",
   "metadata": {},
   "source": [
    "After these experiments, we have found that laplace_smoothing has little to no effect on the model performance, so we will ignore it for the KDE experiments going forward.\n",
    "\n",
    "See `model-selection-results-analysis.ipynb` for the analysis of the first results.\n",
    "\n",
    "#### KDE experiments\n",
    "\n",
    "Note that because KDE models are more costly to evaluate, we will use a simple train-validation split instead of k-fold cross-validation for model selection, and we will also not check multiple values of the laplace_smoothing parameter, instead using the default value of `1e-9` only.\n",
    "\n",
    "> **Note:** The most straight-forward way to run single train/validation split experiments on our existing experiment framework is to set `num_folds=math.ceil(1/validation_split)`, but add a single fold with index 0. In the runtime, the model trainer will then skip all folds except fold 0, effectively performing a single train/validation split, and because the number of folds is set to `math.ceil(1/validation_split)`, the split proportions will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cda7d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian KDE Naive Bayes Variants:\n",
    "import math\n",
    "\n",
    "validation_split = 0.2\n",
    "num_folds = math.ceil(1 / validation_split)\n",
    "experiments: Set[Experiment] = set()\n",
    "\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for bandwidth in [None, 0.05, 0.1, 1]:\n",
    "        for num_points in [100, 1000, 5000]:\n",
    "            experiments.add(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"EagerGaussianKDEstimator\",\n",
    "                    dataset=dataset,\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=0,\n",
    "                    continuous_estimator_params=dict(\n",
    "                        bandwidth=bandwidth, num_points=num_points, range_padding=0.1, batch_size=1000\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "\n",
    "for bandwidth in [None, 0.05, 0.1, 1]:\n",
    "    for num_points in [100, 1000, 5000]:\n",
    "        experiments.add(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustEagerGaussianKDEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=0,\n",
    "                continuous_estimator_params=dict(\n",
    "                    bandwidth=bandwidth,\n",
    "                    num_points=num_points,\n",
    "                    range_padding=0.1,\n",
    "                    batch_size=1000,\n",
    "                    laplace_smoothing=1e-9,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.add(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustEagerGaussianKDEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=0,\n",
    "                continuous_estimator_params=dict(\n",
    "                    bandwidth=bandwidth,\n",
    "                    num_points=num_points,\n",
    "                    range_padding=0.1,\n",
    "                    batch_size=1000,\n",
    "                    laplace_smoothing=1e-9,\n",
    "                ),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f06b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   6%|▋         | 3/48 [14:04<4:35:42, 367.61s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:   8%|▊         | 4/48 [29:36<7:12:58, 590.43s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:15: UserWarning: Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\n",
      "  warnings.warn(\"Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\")\n",
      "c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  33%|███▎      | 16/48 [1:45:47<2:50:34, 319.82s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:15: UserWarning: Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\n",
      "  warnings.warn(\"Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\")\n",
      "c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  42%|████▏     | 20/48 [1:46:38<40:10, 86.10s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]   c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  52%|█████▏    | 25/48 [2:02:48<1:00:10, 156.96s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:15: UserWarning: Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\n",
      "  warnings.warn(\"Not enough data points to compute bandwidth. Using arbitrary value of 1.0.\")\n",
      "c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  58%|█████▊    | 28/48 [2:18:18<1:09:50, 209.54s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  67%|██████▋   | 32/48 [2:29:26<43:48, 164.26s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]  c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  75%|███████▌  | 36/48 [2:47:57<41:37, 208.12s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]  c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  81%|████████▏ | 39/48 [2:51:56<20:18, 135.35s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  85%|████████▌ | 41/48 [2:54:56<12:19, 105.62s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  88%|████████▊ | 42/48 [3:08:33<31:55, 319.25s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments:  96%|█████████▌| 46/48 [3:29:11<09:54, 297.50s/it, CategoricalAwareBespokeNB with RobustEagerGaussianKDEstimator on original]c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\AML\\HiggsBosonATLAS\\src\\naive_bayes\\kde_estimators\\__init__.py:256: UserWarning: All data points are NaN. Density estimation cannot be computed. Using empty density.\n",
      "  warnings.warn(\"All data points are NaN. Density estimation cannot be computed. Using empty density.\")\n",
      "Running experiments: 100%|██████████| 48/48 [3:32:00<00:00, 265.01s/it, BespokeNB with RobustEagerGaussianKDEstimator on original]                \n"
     ]
    }
   ],
   "source": [
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    run_all_experiments(experiments=set(experiments), results_directory=\"../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgsbosonatlas (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
