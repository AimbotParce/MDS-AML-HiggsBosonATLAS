{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eded411",
   "metadata": {},
   "source": [
    "## Naive Bayes Model Selection\n",
    "\n",
    "We have implemented several different variants of the Naive Bayes classifier, each with its own assumptions and characteristics. To determine which model performs best for our specific dataset, we will conduct a model selection process using some resampling technique.\n",
    "\n",
    "### Choice of Resampling Technique\n",
    "\n",
    "By default, we will use k-fold cross-validation for model selection and hyperparameter tuning. However, in some algorithms, training is extremely costly, and in such cases, because we have a large dataset, we may opt for a simple train-validation split instead. The choice of resampling technique will be made based on the computational cost of training each model.\n",
    "\n",
    "Note that in all cases, the test set will remain untouched until the final evaluation phase.\n",
    "\n",
    "### Model Space\n",
    "\n",
    "We will consider the following Naive Bayes variants for model selection:\n",
    "1. Gaussian Naive Bayes Variants:\n",
    "   1. Gaussian Naive Bayes (As a baseline model): The standard Gaussian Naive Bayes model that assumes features are normally distributed.\n",
    "      1. Dropping rows with missing values\n",
    "      2. Dropping features with missing values\n",
    "   2. Robust Gaussian Naive Bayes: A variant of the Gaussian Naive Bayes that uses the missingness of a feature as an additional categorical feature. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Gaussian Naive Bayes with: A variant of the Robust Gaussian Naive Bayes that treats continuous features as independent of one another (as does the standard Naive Bayes model) but dependent on the categorical features (not the missingness indicators). (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "2. Histogram-Based Naive Bayes Variants:\n",
    "   1. Histogram Naive Bayes: A Naive Bayes model that uses histograms to estimate the probability density functions of continuous features.\n",
    "      1. Dropping rows with missing values (`bins in range(10, 1000, 10) + [None]`)\n",
    "      2. Dropping features with missing values (`bins in range(10, 1000, 10) + [None]`)\n",
    "   2. Robust Histogram Naive Bayes: A variant of the Histogram Naive Bayes that incorporates missingness indicators as additional categorical features. (`bins in range(10, 1000, 10) + [None]`, `laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Histogram Naive Bayes: A variant of the Robust Histogram Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`bins in range(10, 1000, 10) + [None]`, `laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "3. Kernel Density Estimation (KDE) Based Naive Bayes Variants:\n",
    "   1. Gaussian KDE Naive Bayes: A Naive Bayes model that uses Kernel Density Estimation with a Gaussian Kernel to estimate the probability density functions of continuous features.\n",
    "      1. Dropping rows with missing values (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`)\n",
    "      2. Dropping features with missing values (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`)\n",
    "   2. Robust Gaussian KDE Naive Bayes: A variant of the KDE Naive Bayes that incorporates missingness indicators as additional categorical features. (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`, `laplace_smoothing in <about_the_best_as_in_previous_models> + [0]`)\n",
    "   3. Categorical-Aware Robust Gaussian KDE Naive Bayes: A variant of the Robust KDE Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`bandwidth in [0.05, 0.1, 1, None]`, `num_points in [100, 1000, 5000]`, `range_padding = 0.1`, `laplace_smoothing in <about_the_best_as_in_previous_models> + [0]`)\n",
    "   > **Note:** KDE models are by design lazily evaluated, meaning that training consists on just storing all the training data, and actual density estimation is performed at prediction time. Therefore, they are extremely costly to evaluate. For that reason, an **approximation** is used instead, where a large-enough sample of the feature axis is drawn and the density is estimated at those points only, using them to approximate the density at prediction time via 1-nearest-neighbor.\n",
    "4. Yeo-Johnson Transformed Gaussian Naive Bayes Variants:\n",
    "   1. Yeo-Johnson Transformed Gaussian Naive Bayes: A Naive Bayes model that applies a Yeo-Johnson transformation to continuous features before modeling them with Gaussian distributions.\n",
    "      1. Dropping rows with missing values\n",
    "      2. Dropping features with missing values\n",
    "   2. Robust Yeo-Johnson Transformed Gaussian Naive Bayes: A variant of the Yeo-Johnson Transformed Gaussian Naive Bayes that incorporates missingness indicators as additional categorical features. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "   3. Categorical-Aware Robust Yeo-Johnson Transformed Gaussian Naive Bayes: A variant of the Robust Yeo-Johnson Transformed Gaussian Naive Bayes that treats continuous features as independent of one another but dependent on the categorical features. (`laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)\n",
    "\n",
    "\n",
    "> **Note:** In all cases, categorical features are modeled using a multi-class generalization of the Bernoulli distribution ($f(x=i|\\vec{p})=p_i$), while continuous features are modeled using different Density Estimation techniques (Gaussian, KDE, or Histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c0ca5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DER_mass_MMC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_transverse_met_lep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_vis",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_deltaeta_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_mass_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_prodeta_jet_jet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_deltar_tau_lep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_tot",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_sum_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_pt_ratio_lep_tau",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_met_phi_centrality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DER_lep_eta_centrality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_tau_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_lep_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_met_sumet",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PRI_jet_leading_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_leading_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_leading_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_eta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_subleading_phi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PRI_jet_all_pt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3c9311f1-9b5c-422e-8106-439925d699b9",
       "rows": [
        [
         "0",
         "90.901",
         "85.57",
         "75.316",
         "40.945",
         null,
         null,
         null,
         "1.869",
         "3.111",
         "135.816",
         "1.073",
         "-1.06",
         null,
         "44.964",
         "-0.972",
         "0.788",
         "48.247",
         "-0.738",
         "-1.066",
         "37.976",
         "2.136",
         "185.052",
         "1",
         "42.605",
         "-1.962",
         "-2.519",
         null,
         null,
         null,
         "42.605",
         "0.58939479024",
         "b"
        ],
        [
         "1",
         "133.477",
         "3.669",
         "99.223",
         "227.121",
         "2.243",
         "365.016",
         "2.278",
         "1.223",
         "3.539",
         "440.917",
         "2.495",
         "0.949",
         "0.094",
         "51.602",
         "-0.978",
         "0.509",
         "128.748",
         "-0.155",
         "-0.395",
         "63.331",
         "-0.436",
         "453.808",
         "2",
         "173.249",
         "-0.759",
         "2.545",
         "87.317",
         "-3.002",
         "-2.594",
         "260.566",
         "0.000461281573949",
         "s"
        ],
        [
         "2",
         "115.111",
         "26.919",
         "77.658",
         "50.266",
         null,
         null,
         null,
         "2.691",
         "3.655",
         "133.495",
         "1.398",
         "1.398",
         null,
         "33.191",
         "-0.614",
         "-1.834",
         "46.409",
         "-0.248",
         "1.783",
         "27.559",
         "2.555",
         "176.401",
         "1",
         "53.895",
         "0.685",
         "-0.613",
         null,
         null,
         null,
         "53.895",
         "0.623626505663",
         "b"
        ],
        [
         "3",
         null,
         "83.642",
         "74.642",
         "25.176",
         null,
         null,
         null,
         "2.646",
         "25.176",
         "53.813",
         "1.393",
         "-1.361",
         null,
         "22.488",
         "0.205",
         "-1.051",
         "31.326",
         "2.215",
         "0.671",
         "60.143",
         "-3.012",
         "77.408",
         "0",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "1.77211428467",
         "b"
        ],
        [
         "4",
         "81.958",
         "7.074",
         "46.894",
         "90.979",
         "0.952",
         "83.883",
         "-0.226",
         "1.626",
         "17.517",
         "174.686",
         "1.05",
         "1.171",
         "0.001",
         "29.799",
         "0.486",
         "2.777",
         "31.299",
         "1.271",
         "1.353",
         "47.727",
         "1.536",
         "265.013",
         "2",
         "80.028",
         "-0.456",
         "-1.902",
         "33.561",
         "0.496",
         "-0.555",
         "113.589",
         "0.418760425129",
         "b"
        ]
       ],
       "shape": {
        "columns": 32,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.901</td>\n",
       "      <td>85.570</td>\n",
       "      <td>75.316</td>\n",
       "      <td>40.945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.869</td>\n",
       "      <td>3.111</td>\n",
       "      <td>135.816</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>42.605</td>\n",
       "      <td>-1.962</td>\n",
       "      <td>-2.519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.605</td>\n",
       "      <td>0.589395</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133.477</td>\n",
       "      <td>3.669</td>\n",
       "      <td>99.223</td>\n",
       "      <td>227.121</td>\n",
       "      <td>2.243</td>\n",
       "      <td>365.016</td>\n",
       "      <td>2.278</td>\n",
       "      <td>1.223</td>\n",
       "      <td>3.539</td>\n",
       "      <td>440.917</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>173.249</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>2.545</td>\n",
       "      <td>87.317</td>\n",
       "      <td>-3.002</td>\n",
       "      <td>-2.594</td>\n",
       "      <td>260.566</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.111</td>\n",
       "      <td>26.919</td>\n",
       "      <td>77.658</td>\n",
       "      <td>50.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.691</td>\n",
       "      <td>3.655</td>\n",
       "      <td>133.495</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.895</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.895</td>\n",
       "      <td>0.623627</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>83.642</td>\n",
       "      <td>74.642</td>\n",
       "      <td>25.176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.646</td>\n",
       "      <td>25.176</td>\n",
       "      <td>53.813</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.772114</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.958</td>\n",
       "      <td>7.074</td>\n",
       "      <td>46.894</td>\n",
       "      <td>90.979</td>\n",
       "      <td>0.952</td>\n",
       "      <td>83.883</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>1.626</td>\n",
       "      <td>17.517</td>\n",
       "      <td>174.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80.028</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>33.561</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>113.589</td>\n",
       "      <td>0.418760</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0        90.901                       85.570        75.316    40.945   \n",
       "1       133.477                        3.669        99.223   227.121   \n",
       "2       115.111                       26.919        77.658    50.266   \n",
       "3           NaN                       83.642        74.642    25.176   \n",
       "4        81.958                        7.074        46.894    90.979   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                   NaN               NaN                  NaN   \n",
       "1                 2.243           365.016                2.278   \n",
       "2                   NaN               NaN                  NaN   \n",
       "3                   NaN               NaN                  NaN   \n",
       "4                 0.952            83.883               -0.226   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  PRI_jet_num  \\\n",
       "0               1.869       3.111     135.816  ...            1   \n",
       "1               1.223       3.539     440.917  ...            2   \n",
       "2               2.691       3.655     133.495  ...            1   \n",
       "3               2.646      25.176      53.813  ...            0   \n",
       "4               1.626      17.517     174.686  ...            2   \n",
       "\n",
       "   PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0              42.605               -1.962               -2.519   \n",
       "1             173.249               -0.759                2.545   \n",
       "2              53.895                0.685               -0.613   \n",
       "3                 NaN                  NaN                  NaN   \n",
       "4              80.028               -0.456               -1.902   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                    NaN                     NaN                     NaN   \n",
       "1                 87.317                  -3.002                  -2.594   \n",
       "2                    NaN                     NaN                     NaN   \n",
       "3                    NaN                     NaN                     NaN   \n",
       "4                 33.561                   0.496                  -0.555   \n",
       "\n",
       "   PRI_jet_all_pt    Weight  Label  \n",
       "0          42.605  0.589395      b  \n",
       "1         260.566  0.000461      s  \n",
       "2          53.895  0.623627      b  \n",
       "3           0.000  1.772114      b  \n",
       "4         113.589  0.418760      b  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and prepare the clean variables.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/train_no_preprocess.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57b61",
   "metadata": {},
   "source": [
    "Let us first define three different data sets that we'll use for the different missing data strategies: one that drops rows with missing values, one that drops features with missing values, and one that retains all data (for the robust models). We'll store them in a dictionary for easy access later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ca716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Label\", \"Weight\"])\n",
    "y = df[\"Label\"]\n",
    "weights = df[\"Weight\"]\n",
    "categorical_features = df.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_drop_rows = df[~X.isna().any(axis=1)].reset_index(drop=True)\n",
    "X_drop_rows = df_drop_rows.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_drop_rows = df_drop_rows[\"Label\"]\n",
    "weights_drop_rows = df_drop_rows[\"Weight\"]\n",
    "categorical_features_drop_rows = df_drop_rows.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "df_drop_cols = df.drop(columns=X.columns[X.isna().any(axis=0)])\n",
    "X_drop_cols = df_drop_cols.drop(columns=[\"Label\", \"Weight\"])\n",
    "y_drop_cols = df_drop_cols[\"Label\"]\n",
    "weights_drop_cols = df_drop_cols[\"Weight\"]\n",
    "categorical_features_drop_cols = df_drop_cols.columns.get_indexer([\"PRI_jet_num\"]).tolist()\n",
    "\n",
    "# Convert them to numpy and store them in a datasets dictionary for easy reference.\n",
    "datasets = {\n",
    "    \"original\": (X.to_numpy(), y.to_numpy(), weights.to_numpy(), categorical_features),\n",
    "    \"drop-rows\": (\n",
    "        X_drop_rows.to_numpy(),\n",
    "        y_drop_rows.to_numpy(),\n",
    "        weights_drop_rows.to_numpy(),\n",
    "        categorical_features_drop_rows,\n",
    "    ),\n",
    "    \"drop-columns\": (\n",
    "        X_drop_cols.to_numpy(),\n",
    "        y_drop_cols.to_numpy(),\n",
    "        weights_drop_cols.to_numpy(),\n",
    "        categorical_features_drop_cols,\n",
    "    ),\n",
    "}\n",
    "del (\n",
    "    df,\n",
    "    X,\n",
    "    y,\n",
    "    df_drop_rows,\n",
    "    X_drop_rows,\n",
    "    y_drop_rows,\n",
    "    df_drop_cols,\n",
    "    X_drop_cols,\n",
    "    y_drop_cols,\n",
    "    categorical_features,\n",
    "    categorical_features_drop_rows,\n",
    "    categorical_features_drop_cols,\n",
    ")  # Free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7922d6",
   "metadata": {},
   "source": [
    "Now let us define an experimenting framework that will allow us to declaratively define which models and hyperparameters to use, and easily run model selection for all the different models and hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d730a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Literal, Union, Tuple, Set, TypeVar, Type\n",
    "from pydantic import BaseModel, model_validator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import src.naive_bayes\n",
    "import src.evaluate\n",
    "\n",
    "\n",
    "class Experiment(BaseModel):\n",
    "    model_class: Literal[\"BespokeNB\", \"CategoricalAwareBespokeNB\"]\n",
    "    categorical_estimator_class: Literal[\"CategoricalEstimator\", \"RobustCategoricalEstimator\"]\n",
    "    continuous_estimator_class: Literal[\n",
    "        \"GaussianEstimator\",\n",
    "        \"RobustGaussianEstimator\",\n",
    "        \"HistogramEstimator\",\n",
    "        \"RobustHistogramEstimator\",\n",
    "        \"EagerGaussianKDEstimator\",\n",
    "        \"RobustEagerGaussianKDEstimator\",\n",
    "        \"YeoJohnsonGaussianEstimator\",\n",
    "        \"RobustYeoJohnsonGaussianEstimator\",\n",
    "    ]\n",
    "    dataset: Literal[\"original\", \"drop-rows\", \"drop-columns\"]\n",
    "    num_folds: int\n",
    "    fold_index: int\n",
    "    categorical_estimator_params: Dict[str, Union[Optional[float], Optional[int]]] = {}\n",
    "    continuous_estimator_params: Dict[str, Union[Optional[float], Optional[int]]] = {}\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_fold_index(self):\n",
    "        if self.fold_index >= self.num_folds:\n",
    "            raise ValueError(\"fold_index must be less than num_folds\")\n",
    "        return self\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.model_class,\n",
    "                self.categorical_estimator_class,\n",
    "                self.continuous_estimator_class,\n",
    "                frozenset(self.categorical_estimator_params.items()),\n",
    "                frozenset(self.continuous_estimator_params.items()),\n",
    "                self.dataset,\n",
    "                self.num_folds,\n",
    "                self.fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Experiment):\n",
    "            return NotImplemented\n",
    "        return (\n",
    "            self.model_class == other.model_class\n",
    "            and self.categorical_estimator_class == other.categorical_estimator_class\n",
    "            and self.continuous_estimator_class == other.continuous_estimator_class\n",
    "            and self.categorical_estimator_params == other.categorical_estimator_params\n",
    "            and self.continuous_estimator_params == other.continuous_estimator_params\n",
    "            and self.dataset == other.dataset\n",
    "            and self.num_folds == other.num_folds\n",
    "            and self.fold_index == other.fold_index\n",
    "        )\n",
    "\n",
    "\n",
    "class ExperimentResult(Experiment):\n",
    "    accuracy: float\n",
    "    b_recall: float\n",
    "    b_precision: float\n",
    "    b_f1_score: float\n",
    "    s_recall: float\n",
    "    s_precision: float\n",
    "    s_f1_score: float\n",
    "    ams_score: float\n",
    "\n",
    "\n",
    "def _instantiate_estimator(\n",
    "    estimator_cls: Type[src.naive_bayes.ProbabilityEstimator],\n",
    "    estimator_params: Dict[str, Union[Optional[float], Optional[int]]],\n",
    ") -> src.naive_bayes.ProbabilityEstimator:\n",
    "    init_kwargs = {}\n",
    "    for key in estimator_cls.__init__.__code__.co_varnames[1:]:\n",
    "        if not key in estimator_params:\n",
    "            raise ValueError(f\"Missing value for estimator parameter: {key}\")\n",
    "        init_kwargs[key] = estimator_params[key]\n",
    "    return estimator_cls(**init_kwargs)\n",
    "\n",
    "\n",
    "def _get_estimator_instances(\n",
    "    experiment: Experiment, num_features: int, categorical_features: list[int]\n",
    ") -> Dict[int, src.naive_bayes.ProbabilityEstimator]:\n",
    "    categorical_estimator_cls = getattr(src.naive_bayes, experiment.categorical_estimator_class)\n",
    "    continuous_estimator_cls = getattr(src.naive_bayes, experiment.continuous_estimator_class)\n",
    "    instances = {}\n",
    "    for feature in range(num_features):\n",
    "        if feature in categorical_features:\n",
    "            instances[feature] = _instantiate_estimator(\n",
    "                categorical_estimator_cls, experiment.categorical_estimator_params\n",
    "            )\n",
    "        else:\n",
    "            instances[feature] = _instantiate_estimator(\n",
    "                continuous_estimator_cls, experiment.continuous_estimator_params\n",
    "            )\n",
    "    return instances\n",
    "\n",
    "\n",
    "def _get_model_instance(\n",
    "    experiment: Experiment, num_features: int, categorical_features: list[int]\n",
    ") -> src.naive_bayes.BespokeNB | src.naive_bayes.CategoricalAwareBespokeNB:\n",
    "    model_cls = getattr(src.naive_bayes, experiment.model_class)\n",
    "    estimators = _get_estimator_instances(\n",
    "        experiment, num_features=num_features, categorical_features=categorical_features\n",
    "    )\n",
    "    if model_cls == src.naive_bayes.BespokeNB:\n",
    "        return model_cls(estimators=estimators)\n",
    "    elif model_cls == src.naive_bayes.CategoricalAwareBespokeNB:\n",
    "        return model_cls(\n",
    "            estimators=estimators,\n",
    "            categorical_features=categorical_features,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model class: {experiment.model_class}\")\n",
    "\n",
    "\n",
    "def run_experiment(experiment: Experiment) -> ExperimentResult:\n",
    "    X, y, weights, categorical_features = datasets[experiment.dataset]\n",
    "\n",
    "    # Create cross-validation folds\n",
    "    kf = KFold(n_splits=experiment.num_folds, shuffle=True, random_state=42)\n",
    "    for fold_index, (train, test) in enumerate(kf.split(X)):\n",
    "        if fold_index == experiment.fold_index:\n",
    "            # Skip folds that are not the current fold index, and keep only the desired fold\n",
    "            break\n",
    "\n",
    "    model = _get_model_instance(experiment, num_features=X.shape[1], categorical_features=categorical_features)\n",
    "    model.fit(X[train], y[train])\n",
    "    y_pred = model.predict(X[test])\n",
    "    accuracy = accuracy_score(y[test], y_pred)\n",
    "    (b_precision, s_precision), (b_recall, s_recall), (b_f1_score, s_f1_score), _ = precision_recall_fscore_support(\n",
    "        y[test], y_pred, labels=[\"b\", \"s\"], average=None, zero_division=0\n",
    "    )\n",
    "    ams_score = src.evaluate.ams_score(y_true=y[test], y_pred=y_pred, weights=weights[test])\n",
    "    return ExperimentResult(\n",
    "        **experiment.model_dump(),\n",
    "        accuracy=accuracy,\n",
    "        b_recall=b_recall,\n",
    "        b_precision=b_precision,\n",
    "        b_f1_score=b_f1_score,\n",
    "        s_recall=s_recall,\n",
    "        s_precision=s_precision,\n",
    "        s_f1_score=s_f1_score,\n",
    "        ams_score=ams_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def store_experiment_set(experiments: Set[ExperimentResult] | Set[Experiment], filename: str) -> None:\n",
    "    \"\"\"Store a set of ExperimentResult to a JSONL file.\"\"\"\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    with open(filename, \"w\") as f:\n",
    "        for er in experiments:\n",
    "            if not isinstance(er, (ExperimentResult, Experiment)):\n",
    "                raise ValueError(f\"Invalid type in experiments set: {type(er)}\")\n",
    "            f.write(er.model_dump_json() + \"\\n\")\n",
    "\n",
    "\n",
    "def append_to_experiment_set(experiment: ExperimentResult | Experiment, filename: str) -> None:\n",
    "    \"\"\"Append an ExperimentResult to a JSONL file.\"\"\"\n",
    "    if not isinstance(experiment, (ExperimentResult, Experiment)):\n",
    "        raise ValueError(f\"Invalid type in experiments set: {type(experiment)}\")\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(experiment.model_dump_json() + \"\\n\")\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\", bound=Union[ExperimentResult, Experiment])\n",
    "\n",
    "\n",
    "def load_experiment_set(cls: Type[T], filename: str) -> Set[T]:\n",
    "    \"\"\"Load a set of ExperimentResult from a JSONL file.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    experiments: Set[T] = set()\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            experiments.add(cls.model_validate_json(line.strip()))\n",
    "    return experiments\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "    experiments: Set[Experiment],\n",
    "    *,\n",
    "    verbose: bool = True,\n",
    "    results_file: Optional[os.PathLike] = None,\n",
    "    failed_file: Optional[os.PathLike] = None,\n",
    ") -> Tuple[Set[ExperimentResult], Set[Experiment]]:\n",
    "    results: Set[ExperimentResult] = set()\n",
    "    failed: Set[Experiment] = set()\n",
    "    if results_file is not None:\n",
    "        already_ran: Set[ExperimentResult] = load_experiment_set(ExperimentResult, results_file)\n",
    "    if failed_file is not None:\n",
    "        already_failed = load_experiment_set(Experiment, failed_file)\n",
    "    missing_experiments = experiments - already_ran  # Small optimization: If it was ran successfully already, skip it.\n",
    "    bar = tqdm(total=len(missing_experiments), desc=\"Running experiments\", disable=not verbose)\n",
    "    try:\n",
    "        for experiment in missing_experiments:\n",
    "            # Skip experiments that have already been run\n",
    "            if experiment in results:\n",
    "                bar.update(1)\n",
    "                continue\n",
    "            bar.set_postfix_str(\n",
    "                f\"{experiment.model_class} with {experiment.continuous_estimator_class} on {experiment.dataset}\"\n",
    "            )\n",
    "            bar.refresh()\n",
    "            try:\n",
    "                new_result = run_experiment(experiment)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                traceback.print_exc()\n",
    "                warnings.warn(f\"Experiment failed: {e}\")\n",
    "                failed.add(experiment)\n",
    "                if failed_file is not None and experiment not in already_failed:\n",
    "                    append_to_experiment_set(experiment, failed_file)\n",
    "            else:\n",
    "                results.add(new_result)\n",
    "                if results_file is not None:\n",
    "                    append_to_experiment_set(new_result, results_file)\n",
    "            finally:\n",
    "                bar.update(1)\n",
    "    finally:\n",
    "        bar.close()\n",
    "    return results, failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730dfae",
   "metadata": {},
   "source": [
    "Now we're equipped to run the experiments, let's first define them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edbcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments: List[Experiment] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48719d",
   "metadata": {},
   "source": [
    "#### Gaussian experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d78908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                continuous_estimator_class=\"GaussianEstimator\",\n",
    "                dataset=dataset,\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49789e2c",
   "metadata": {},
   "source": [
    "#### Histogram experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bda74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for bins in list(range(10, 1000, 10)) + [None]:\n",
    "        for fold_index in range(num_folds):\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"HistogramEstimator\",\n",
    "                    dataset=dataset,\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(bins=bins),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for bins in list(range(10, 1000, 10)) + [None]:\n",
    "        for fold_index in range(num_folds):\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"RobustHistogramEstimator\",\n",
    "                    dataset=\"original\",\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing, bins=bins),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            experiments.append(\n",
    "                Experiment(\n",
    "                    model_class=\"CategoricalAwareBespokeNB\",\n",
    "                    categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"RobustHistogramEstimator\",\n",
    "                    dataset=\"original\",\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=fold_index,\n",
    "                    continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing, bins=bins),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632ae49",
   "metadata": {},
   "source": [
    "Let us skip the KDE estimators, as they are much much more computationally expensive to run, and perform first the Yeo-Johnson experiments. After that, we can explore the temporary results, and narrow down which hyperparameters to explore for the KDE models.\n",
    "\n",
    "#### Yeo-Johnson experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bd5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeo-Johnson Gaussian Naive Bayes Variants:\n",
    "num_folds = 10\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                continuous_estimator_class=\"YeoJohnsonGaussianEstimator\",\n",
    "                dataset=dataset,\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "for laplace_smoothing in [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for fold_index in range(num_folds):\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustYeoJohnsonGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.append(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustYeoJohnsonGaussianEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=fold_index,\n",
    "                continuous_estimator_params=dict(laplace_smoothing=laplace_smoothing),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4f42f",
   "metadata": {},
   "source": [
    "And finally, let's run the experiments and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f288993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   2%|▎         | 5/200 [00:14<10:09,  3.13s/it, CategoricalAwareBespokeNB with RobustYeoJohnsonGaussianEstimator on original]"
     ]
    }
   ],
   "source": [
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    run_all_experiments(\n",
    "        experiments=set(experiments),\n",
    "        results_file=\"../results/experiments-results.jsonl\",\n",
    "        failed_file=\"../results/experiments-failed.jsonl\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd01fd6",
   "metadata": {},
   "source": [
    "Before running the KDE experiments, let us explore the effects of the laplace smoothing hyperparameter on the previous models, to narrow down the search space for the KDE models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420006b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "res = []\n",
    "with open(\"../results/experiments-results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        res.append(json.loads(line))\n",
    "# Convert to DataFrame for easier analysis\n",
    "res = pd.DataFrame(res)\n",
    "categorical_cols = [\"model_class\", \"categorical_estimator_class\", \"continuous_estimator_class\", \"dataset\"]\n",
    "for col in categorical_cols:\n",
    "    res[col] = res[col].astype(\"category\")\n",
    "# Let us explore the dependency of the scores with the laplace_smoothing parameter.\n",
    "\n",
    "\n",
    "# Now let's create a column with the laplace_smoothing value for easier plotting.\n",
    "def extract_laplace_smoothing(params: dict) -> Optional[float]:\n",
    "    return params.get(\"laplace_smoothing\", None)\n",
    "\n",
    "\n",
    "res[\"laplace_smoothing\"] = res[\"continuous_estimator_params\"].apply(extract_laplace_smoothing)\n",
    "\n",
    "# First, filter the results for the relevant experiments (those Robust or CategoricalAware).\n",
    "laplace_results = res[\n",
    "    (res[\"continuous_estimator_class\"].str.contains(\"Robust\")) | (res[\"model_class\"].str.contains(\"CategoricalAware\"))\n",
    "]\n",
    "\n",
    "laplace_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can plot the AMS score against the laplace_smoothing parameter for different model configurations.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "laplace_results[\"group\"] = laplace_results.apply(\n",
    "    lambda row: f\"{row['model_class']} | {row['continuous_estimator_class']} | {row['dataset']}\", axis=1\n",
    ")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "sns.violinplot(data=laplace_results, x=\"laplace_smoothing\", y=\"ams_score\", hue=\"group\", ax=ax[0], legend=False)\n",
    "ax[0].set_xlabel(\"Laplace Smoothing\")\n",
    "ax[0].set_ylabel(\"AMS Score\")\n",
    "ax[0].set_title(\"AMS Score vs Laplace Smoothing for Different Model Configurations\")\n",
    "\n",
    "sns.violinplot(data=laplace_results, x=\"laplace_smoothing\", y=\"accuracy\", hue=\"group\", ax=ax[1], legend=True)\n",
    "ax[1].set_xlabel(\"Laplace Smoothing\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy vs Laplace Smoothing for Different Model Configurations\")\n",
    "# Disable the legend, as we will create a custom one below\n",
    "ax[1].legend_.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "# Add create the custom legend\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=2, loc=\"lower left\", bbox_to_anchor=(0, -0.11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5a569",
   "metadata": {},
   "source": [
    "After these experiments, we have found that laplace_smoothing has little to no effect on the model performance, so we will ignore it for the KDE experiments going forward.\n",
    "\n",
    "#### KDE experiments\n",
    "\n",
    "Note that because KDE models are more costly to evaluate, we will use a simple train-validation split instead of k-fold cross-validation for model selection, and we will also not check multiple values of the laplace_smoothing parameter, instead using the default value of `1e-9` only.\n",
    "\n",
    "> **Note:** The most straight-forward way to run single train/validation split experiments on our existing experiment framework is to set `num_folds=math.ceil(1/validation_split)`, but add a single fold with index 0. In the runtime, the model trainer will then skip all folds except fold 0, effectively performing a single train/validation split, and because the number of folds is set to `math.ceil(1/validation_split)`, the split proportions will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian KDE Naive Bayes Variants:\n",
    "import math\n",
    "\n",
    "validation_split = 0.2\n",
    "num_folds = math.ceil(1 / validation_split)\n",
    "experiments: Set[Experiment] = set()\n",
    "\n",
    "# Standard\n",
    "for dataset in [\"drop-rows\", \"drop-columns\"]:\n",
    "    for bandwidth in [None, 0.05, 0.1, 1]:\n",
    "        for num_points in [100, 1000, 5000]:\n",
    "            experiments.add(\n",
    "                Experiment(\n",
    "                    model_class=\"BespokeNB\",\n",
    "                    categorical_estimator_class=\"CategoricalEstimator\",\n",
    "                    continuous_estimator_class=\"EagerGaussianKDEstimator\",\n",
    "                    dataset=dataset,\n",
    "                    num_folds=num_folds,\n",
    "                    fold_index=0,\n",
    "                    continuous_estimator_params=dict(\n",
    "                        bandwidth=bandwidth, num_points=num_points, range_padding=0.1, batch_size=1000\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Robust and categorical-aware\n",
    "\n",
    "for bandwidth in [None, 0.05, 0.1, 1]:\n",
    "    for num_points in [100, 1000, 5000]:\n",
    "        experiments.add(\n",
    "            Experiment(\n",
    "                model_class=\"BespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustEagerGaussianKDEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=0,\n",
    "                continuous_estimator_params=dict(\n",
    "                    bandwidth=bandwidth,\n",
    "                    num_points=num_points,\n",
    "                    range_padding=0.1,\n",
    "                    batch_size=1000,\n",
    "                    laplace_smoothing=1e-9,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experiments.add(\n",
    "            Experiment(\n",
    "                model_class=\"CategoricalAwareBespokeNB\",\n",
    "                categorical_estimator_class=\"RobustCategoricalEstimator\",\n",
    "                continuous_estimator_class=\"RobustEagerGaussianKDEstimator\",\n",
    "                dataset=\"original\",\n",
    "                num_folds=num_folds,\n",
    "                fold_index=0,\n",
    "                continuous_estimator_params=dict(\n",
    "                    bandwidth=bandwidth,\n",
    "                    num_points=num_points,\n",
    "                    range_padding=0.1,\n",
    "                    batch_size=1000,\n",
    "                    laplace_smoothing=1e-9,\n",
    "                ),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    run_all_experiments(\n",
    "        experiments=set(experiments),\n",
    "        results_file=\"../results/experiments-results.jsonl\",\n",
    "        failed_file=\"../results/experiments-failed.jsonl\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgsbosonatlas (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
